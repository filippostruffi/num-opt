## Datasets (by task)

- Image Classification
  - MNIST
    - Type: 28×28 grayscale digits, 10 classes
    - Splits: 60k train / 10k test (we use test as validation)
    - Format: TorchVision `datasets.MNIST` (auto-download)
    - Preprocess: ToTensor only (values [0,1])
    - Why: Extremely fast for sanity checks and optimizer sweeps
    - Pitfalls: Single channel; models with heavy regularization may underfit fast on only a few samples
  - CIFAR10
    - Type: 32×32 color images, 10 classes
    - Splits: 50k train / 10k test (we use test as validation)
    - Format: TorchVision `datasets.CIFAR10`
    - Preprocess: ToTensor only by default (you can add normalization/augmentation externally)
    - Why: Standard small-scale vision benchmark with modest difficulty
    - Pitfalls: Without normalization, convergence can vary by optimizer; batch size affects BN stability

- Semantic Segmentation
  - Oxford-IIIT Pet
    - Type: Pet images with instance masks converted to semantic masks
    - Splits: ~3.7k train / ~3.7k val
    - Format: TorchVision `OxfordIIITPet` with `target_types='segmentation'`
    - Labels: Typically background+class, ignore index=255 used in loss
    - Why: Manageable size; quick dense labeling runs
    - Pitfalls: Class imbalance; drop_last=True recommended to keep BN stable
  - ADE20K
    - Type: Scene parsing, 150 semantic classes
    - Splits: 20k train / 2k val (we subsample via `--max-samples`)
    - Format: HF `scene_parse_150`
    - Preprocess: Images/masks aligned; ignore_index=255
    - Why: Rich label space stresses dense predictions and optimizers
    - Pitfalls: Small batches cause noisy BN; use batch size >1, drop_last=True

- Sentiment Analysis
  - IMDB
    - Type: Binary sentiment (pos/neg) for movie reviews
    - Splits: 25k/25k; long text sequences
    - Format: HF `imdb`
    - Tokenization: Either simple whitespace (for LSTM) or HF tokenizer (DistilBERT)
    - Why: Long-form text magnifies tokenization/optimizer dynamics
    - Pitfalls: Very long reviews benefit from truncation; consider max_len
  - SST2 (GLUE)
    - Type: Binary sentiment for short sentences
    - Splits: ~67k train / 1.8k val (via GLUE)
    - Format: HF `glue`, subset `sst2`
    - Why: Very fast runs and common baseline
    - Pitfalls: Domain shortness may saturate accuracy quickly on tiny samples

- Machine Translation
  - Europarl Bilingual (en-de)
    - Type: Parallel en–de sentences
    - Splits: Provided by HF with config `de-en`
    - Format: HF `europarl_bilingual`, `de-en`
    - Collation: Teacher forcing (tgt_in = tgt[:-1], tgt_out = tgt[1:])
    - Why: Clean parallel data; stable for seq2seq optimizer tests
    - Pitfalls: Vocabulary quality affects BLEU; ensure consistent tokenization
  - IWSLT14 en-de (opus_books proxy)
    - Type: Small MT benchmark (here via HF `opus_books`, `de-en`, mirroring small-scale MT)
    - Format: HF `opus_books`, `de-en`
    - Why: Lightweight alternative to classic IWSLT14 for quick sweeps
    - Pitfalls: Domain differences from original IWSLT14; small size can overfit quickly

- Named Entity Recognition
  - CoNLL2003
    - Type: English newswire NER (PER/LOC/ORG/MISC + O)
    - Splits: train/val/test provided
    - Format: HF `conll2003`
    - Features: We build word vocabulary; tags numeric or mapped; CRF decoding in models
    - Why: Canonical NER; stable F1 comparisons
    - Pitfalls: Tag mapping consistency; mask lengths must match tokens
  - WikiANN (en)
    - Type: NER on Wikipedia (English subset)
    - Splits: train/val/test provided
    - Format: HF `wikiann`, subset `en`
    - Features: Word vocab + optional char vocab; good for char-CNN ablations
    - Why: Diverse entities; stresses generalization
    - Pitfalls: Tokenization variation; max_len truncation can cut entities

- Text Generation (Language Modeling)
  - WikiText2
    - Type: Word-level LM on Wikipedia text
    - Splits: train/val provided
    - Format: HF `wikitext`, subset `wikitext-2-raw-v1`
    - Preprocess: Whitespace tokenization, block packing (default block_size=64)
    - Why: Widely used LM baseline; perplexity is a sensitive metric
    - Pitfalls: Whitespace tokenization is simple; vocab cutoff affects OOV rate
  - PTB (ptb_text_only)
    - Type: Word-level LM; compact
    - Loading: HF `text` dataset with remote PTB URLs (train/valid)
    - Preprocess: Whitespace tokenization; block packing
    - Why: Extremely fast to iterate; good for optimizer ablations
    - Pitfalls: Classic preprocessing removes punctuation; vocab smaller than modern corpora

- Text Summarization
  - CNN/DailyMail (3.0.0)
    - Type: News article summarization to highlights
    - Splits: Standard 3.0.0 splits (HF)
    - Format: HF `cnn_dailymail`, version `3.0.0`
    - Preprocess: Simple word-level vocab for src/tgt; BOS/EOS
    - Why: Canonical summarization dataset; strong baseline
    - Pitfalls: Long inputs benefit from larger max_len than tiny runs
  - AESLC
    - Type: Email body → subject line summarization
    - Splits: Provided by HF `aeslc`
    - Format: Short target sequences; fast training
    - Why: Quick optimizer sweeps; small targets amplify optimization differences
    - Pitfalls: Very short outputs can saturate metrics; label smoothing can stabilize

- Question Answering (extractive)
  - SQuAD v1
    - Type: Span extraction from Wikipedia passages
    - Splits: train/validation
    - Format: HF `squad`
    - Preprocess: Whitespace tokens; we compute start/end via exact token match
    - Why: Standard QA baseline with clean supervision
    - Pitfalls: Tokenization mismatch can miss spans; ensure lowercasing consistency
  - TweetQA
    - Type: QA over tweets (short, noisy text)
    - Splits: train/validation/test
    - Format: HF `tweet_qa` (keys `Tweet`/`Question`/`Answer`)
    - Preprocess: Normalize field names; span matching via token overlap
    - Why: Very fast and complementary to SQuAD domain
    - Pitfalls: Informal language; exact match spans may be rare—small-sample F1 can be low

Selection rationale
- Widely used, accessible via TorchVision/HF
- Sized for rapid optimizer sweeps (further reducible via `--max-samples`)
- Cover diverse modalities (vision/NLP), label structures (classification, spans, dense), and sequence types (LM, seq2seq)


